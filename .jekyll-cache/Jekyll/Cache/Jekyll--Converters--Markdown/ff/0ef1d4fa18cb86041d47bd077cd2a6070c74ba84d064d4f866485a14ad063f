I"<h4 id="论文信息">论文信息</h4>

<blockquote>
  <p>题名: Transfer Learning for Low-Resource Neural Machine Translation</p>

  <p>来源: EMNLP</p>

  <p>年份: 2016</p>

  <p>学习原因: 学习AAAI新的论文中的迁移学习方法</p>
</blockquote>

<ul>
  <li>
    <h5 id="1-abstract"><a href="#1">1 Abstract</a></h5>
  </li>
  <li>
    <h5 id="2-introduction"><a href="#2">2 Introduction</a></h5>
  </li>
  <li>
    <h5 id="3-method"><a href="#3">3 Method</a></h5>
  </li>
  <li>
    <h5 id="4-experiments"><a href="#4">4 Experiments</a></h5>
  </li>
  <li>
    <h5 id="5-conclusions--contibutions"><a href="#5">5 Conclusions &amp; Contibutions</a></h5>
  </li>
</ul>

<hr />

<p><br /></p>

<h4 id="1">摘要</h4>

<p>在语料丰富的情况下，基于encoder-decoder结构的NMT非常强力有效，但是在低资源语言的翻译下就不再那么好用了。作者提出用资源丰富的语料对训练parent model，然后将所学到的一些参数迁移到低资源的模型(child model)中。用这种方法，他们将原有的NMT model baseline的BLEU值提升了5.6，通过未知词替换也提高了BLEU值2点，并且其性能可以SBMT相提并论，并且在其中一种低资源语言的语言对上的翻译超过了SBMT。他们同时发现通过NMT计算SBMT中的最终分数能够帮助SBMT的翻译得到提升，可以达到SOTA。</p>

<p><br /></p>

<h4 id="2">Introduction</h4>

<p>作者首次将Transfer Learning应用到了翻译当中。主要的思想是训练一个高资源平行语料对的翻译模型（parent model）去初始化低资源平行语料模型（child model）的参数。最后固定parent model的特定参数对剩余的参数进行微调得到最终模型。</p>

<p>作者发现这种方法不仅能提高原有NMT baseline的BLEU值，还能通过对SOTA的SBMT模型做re-score，来提升其翻译效果。</p>

<p><br /></p>

<h4 id="3">Method</h4>

<p>结构：两层RNN LSTM（<code class="language-plaintext highlighter-rouge">当时还没有Transformer哈哈</code>）</p>

<p>数据特征：训练集规模是真的很小（0.2M），以及来自众多不同领域。</p>

<p><img src="http://0.0.0.0:4000/images/p2-table2.png" alt="img" class="center-image" /><em>Table1 低资源语料大小</em></p>

<p>方法：</p>

<ol>
  <li>用双向资源都丰富的平行语料训练NMT（法-英），得到parent model。</li>
  <li>用得到的parent model初始化一个新的NMT的参数，称为child model</li>
  <li>用低资源的平行语料训练child model（低-英）（<code class="language-plaintext highlighter-rouge">是否代表低资源的一定要当源语言这样训练？</code>）<br />
<strong>fine-tune阶段</strong>：对一些参数固定，另一些进行fine-tune，可以看作是对参数空间里的参数进行正则化的近似。</li>
</ol>

<p><br /></p>

<h4 id="4">实验</h4>

<p><strong>Transfer Learning</strong></p>

<p><img src="http://0.0.0.0:4000/images/p2-table1.png" alt="img" class="center-image" /><em>Table2 迁移学习实验</em></p>

<p>通过作者提出的迁移学习方法训练出的模型<code class="language-plaintext highlighter-rouge">Xfer</code>对比baseline有了较大的提升（5.6 BLEU）。通过位知词替换方法，最终模型<code class="language-plaintext highlighter-rouge">Final</code>的平均BLEU值提升了7.5，并在一种语言（Hausa）翻译的表现上超过了SBMT。</p>

<p><br /></p>

<p><strong>Re-scoring</strong></p>

<p><img src="http://0.0.0.0:4000/images/p2-table3.png" alt="img" class="center-image" /><em>Tabel3 SBMT的Re-score实验</em></p>

<p>当对SBMT系统的输出n个最佳列表（n = 1000）做Re-scoring时，作者使用这个带有转移学习功能的NMT模型。（<code class="language-plaintext highlighter-rouge">不太了解SBMT的这一步</code>）</p>

<p>其中的<code class="language-plaintext highlighter-rouge">LM</code>是作为没有经过transfer的NMT model作对照的语言模型。</p>

<p><br /></p>

<p><strong>Fine-tune &amp; Fronzen</strong></p>

<p>在进行child model的训练时，初始化参数时会将一些参数（可以在新模型中延续使用的）frozen，比如英文的Embedding，其它参数进行fine-tune，作者比较了几种fine-tune和frozen不同参数的组合策略，作为消融实验，提升效果如图。</p>

<p><img src="http://0.0.0.0:4000/images/p2-table4.png" alt="img" class="center-image" /><em>Table4 Ablation</em></p>

<h4 id="5">结论与贡献</h4>

<p>通过作者继续做的一堆实验，分析出Transfer的一些规律：</p>

<ul>
  <li>当parent model 中的高资源语料和child model中的语料在语言学上更相似时，transfer的效果更好。</li>
  <li>parent model中的语言选择对child model的模型效果影响至关重要。</li>
  <li>通过Transfer learning，child model学到的不仅仅是英语的语言模型，还能通过parent的大量双语文本学到其一些可以利用的翻译参数。（<code class="language-plaintext highlighter-rouge">玄学</code>）</li>
</ul>

<p><strong>贡献</strong></p>

<ul>
  <li>提升了NMT baseline的BLEU</li>
  <li>通过迁移学习的模型做re-scoring提升了SBMT的BLEU</li>
</ul>
:ET