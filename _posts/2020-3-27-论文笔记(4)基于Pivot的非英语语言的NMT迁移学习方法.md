---
layout: post
title: "论文笔记(4)利用基于跨语言预训练的迁移学习方法进行零标注的神经机器翻译"
date: 2020-03-27 12:15:55
description: Pivot-based Transfer Learning for Neural Machine Translation between Non-English Languages
tags: 
 - Machine Translation
 - NLP
 - Low Resource
 - Transfer learning
 - Pivot 
 - 论文笔记
---



#### 论文信息

> 题名: Pivot-based Transfer Learning for Neural Machine Translation between Non-English Languages
>
> 来源: EMNLP
>
> 年份: 2019
>
> 学习原因: 学习AAAI2020的那篇迁移学习有提到

* ##### [1 Abstract](#1)

* ##### [2 Introduction](#2)

* ##### [3 Method](#3)

* ##### [4 Experiments](#4)

* ##### [5 Conclusions & Contibutions](#5)

---

<h4 id='1'>摘要</h4>
改进了低资源语言迁移学习的方法，提出了三种新的基于Pivot的迁移学习方法。

(`标题看见non-english以为终于遇见以中文为研究对象的pivot-based的方法了，但其实不是。这里的non-english主要是想表明除了英文以外的语言没有大量的语料`)

<br>

<h4 id='2'>Introduction</h4>
作者提出，pivot-based的翻译方法有两个很明显的缺点：

1.解码时间为直接翻译的两倍。

2.生成的错误会在pivoting的过程中传播。

所以作者认为还是直接建立source→target的翻译模型比较好。（`显而易见`）

相关工作：

1.基于pivot方法合成语料。(`下次会看这一篇`)

2.多语言翻译模型训练。

作者相应的也通过迁移学习的思想，创建了一个直接的nmt模型。（`虽然使用了pivot的方法，但并不是pivot直接对语料进行翻译训练，而是进行参数拷贝，所以作者认为根本上还是一个直接的模型吧`）。

---

相关概念：

1. `pivot translation`(pivoting)

   就是普通的source→pivot,pivot→target这样翻译。

2. 利用pivot-based方法合成平行语料。

   (之后看)

3. pivot-based模型训练。

   基于source→pivot模型与pivot→target模型的特征训练source→target模型。

   作者也是用的这种方法进行迁移学习，这种方法使用source→pivot和pivot→target的平行语料做预训练，而使用source→target的语料进行微调（`pre-training & fine-tuning`）。

   <br>

<h4 id='3'>Method</h4>
##### 一般模式

首先先看看一般的pivot-based迁移学习的模式。

![img]({{ site.baseurl | prepend:site.url}}/images/p4-pic1.png){: .center-image }*普通的迁移学习*

训练source→pivot，pivot→target的模型，然后用对应的参数初始化source encoder和target decoder，再用(low resource)或者不用(zero-shot)source→target的语料继续训练模型。

在这里的时候，我想起了最初把迁移学习应用在机器翻译领域的那篇文章Zoph所提到的方法并不是这样，根据回顾可以大概做出以下推断：

Zoph的可能比较早了，和他文中提到的普通模型都不一样，Zoph是高资源->target，copy encoder参数到低资源->target模型，再用少量的平行语料训练。

而这种pivot based主要思路可能是做zero-shot属于平行语料真的很少以及没有。

---

之后作者就提出了自己的方法，针对如何解决source→pivot,以及pivot→target直接的不一致性。

看了上一篇论文的话会觉得这篇论文提出的这些方法，本质上还是在解决pivot model参数的过程中，source和pivot的语义空间不匹配的问题。作者提出了三种方法，不全是应用在预训练阶段。

##### 阶梯式的预训练

![img]({{ site.baseurl | prepend:site.url}}/images/p4-pic2.png){: .center-image }*阶梯式的预训练*

1. 训练source→pivot的nmt
2. copy 1中encoder的参数，用pivot→target的语料训练模型
   利用了联合词典训练source和pivot

在拷贝参数的方面来说这种方法与一般模式相比使得source encoder和pivot encoder建立了联系。

##### 枢适配器(Pivot Adapter)

1. 用source→pivot的模型encode source的句子

2. 用pivot→target的模型encode pivot的句子
3. 池化，训练一个映射M，将source和pivot的表示映射到一个空间
4. 通过M，用source和target的语料训练模型

`如果是zero-shot可能就用不了这个方法了吧`

##### 跨语言编码器

通过修改source→pivot的训练过程使得encoder拥有跨语言的性能。也就是同时也用pivot做输入训练模型，但当然不能直接输入，而是运用了去噪自编码器的思想，将原始的句子加入了噪声。

---

总的来说：

- 跨语言编码器对Pre-train阶段的Encoder做了优化。
- 阶梯式的预训练对Pre-train阶段的Decoder做了优化。

- 枢适配器是对训练过程进行优化。

<h4 id='4'>实验</h4>
> PreProcessing & Configuration

- 英语作为pivot language
- 用Moses对所有语料进行了分词和true-casing(将单词转换为最有可能的原型)。
- 每种语言32K的BPE，训练跨语言encoder时使用的是joint BPE
- 训练的模型为6层的Transformer

> 性能 - Low Resource的情况

![img]({{ site.baseurl | prepend:site.url}}/images/p4-table1.png){: .center-image }*结果*

- 多语言的训练方式比单独的encoder、decoder训练的成果要好

- Many-to-many 比 many-to-one的效果要好

- plain transfer对比直接训练提升很大，pivot adapter更是使其获得了更大的性能提升

- 跨语言的encoder的预训练没什么用，和plain transfer没差多少，加了Pivot adapter后效果提升。

  作者认为是跨语言的encoder需要更多的数据去为新的decoder做fine-tune，而且encoder的两种语言也分隔的很开（`不是很懂，不是joint training吗`），而pivot adapter直接缩小了两种语言的差距，并且decoder都是同一个（`训练pivot adpter用的是source→pivot的双语语料，比较多，效果合理`）。

- 阶梯式训练的提升最大（+1.2%BLEU），加了跨语言encoder后提升更高。

Pivot adapter由于在encoder和decoder中加了一层，所以并不适合将其添加进阶梯式训练中，因为阶梯式训练的decoder已经和pre-train阶段的encoder训练好了，加入pivot adapter反而有害性能。

> 性能 - Zero-shot的情况

![img]({{ site.baseurl | prepend:site.url}}/images/p4-table2.png){: .center-image }*结果*

- Plain transfer几乎玩完

- 阶梯式训练有效地解决了plain transfer的痛点(decoder已经train过一遍)，加入跨语言encoder后效果也得到了提升，作者又引入了Chen（`之后看这个`）提出的teacher-student模型，效果得到了更大的提升。

  （`这引入teacher-student之后效果和low-resource的不相上下，既然如此为什么不在之前的实验中加入这个方式？`）

> 性能 - 大规模数据的情景

拥有更多的真实source→target以及合成的source→target语料。

![img]({{ site.baseurl | prepend:site.url}}/images/){: .center-image }*结果*

- Plain transfer挺厉害
- 作者的方法提升不大，因为语料充足的时候，作者认为plain transfer已经达到了不错的fine-tune效果。

<h4 id='5'>结论与贡献</h4>
作者主要提出了三种方法，在NMT中能够使基于pivot的tranfer Learning方法取得更好的效果。

> **思考**

1. 作者对几种pivot相关的方法，什么是间接模型什么是直接模型讲得很清楚的，图也很直观。对pivot-based model trainning的解释总结也很到位。

2. 可能可以用到中文里，如果是pivot adaptor和阶梯式预训练的话应该也不需要字符集统一吧。

   