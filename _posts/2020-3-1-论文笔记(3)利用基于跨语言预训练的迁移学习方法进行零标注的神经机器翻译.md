---
layout: post
title: "论文笔记(3)利用基于跨语言预训练的迁移学习方法进行零标注的神经机器翻译"
date: 2020-03-01 11:48:18
description: Cross-lingual Pre-training Based Transfer for Zero-shot Neural Machine Translation
tags: 
 - Machine Translation
 - NLP
 - Low Resource
 - Transfer learning
 - Zero-shot
 - 论文笔记
---



#### 论文信息

> 题名: Cross-lingual Pre-training Based Transfer for Zero-shot Neural Machine Translation
>
> 来源: AAAI
>
> 年份: 2020
>
> 学习原因: 组会备选

* ##### [1 Abstract](#1)

* ##### [2 Introduction](#2)

* ##### [3 Method](#3)

* ##### [4 Experiments](#4)

* ##### [5 Conclusions & Contibutions](#5)

---



<h4 id='1'>摘要</h4>
不同语言对之间的迁移学习在低资源场景下对神经机器翻译（NMT）具有一定效果。然而，由于源端的迁移对象（父模型）和被迁移对象（子模型）之间的`语义空间不匹配`问题，目前的迁移方法在零标注等极端场景下并不有效。为了解决该问题，作者提出一种全新的基于跨语言预训练的迁移方法。关键思想是让所有源语言`共享相同的特征空间`，从而实现零标注翻译的平稳过渡。作者引入了一种仅基于单语和两种基于双语的跨语言预训练方法，以获得适用于不同语言的通用encoder。进一步地，作者利用该通用编码器在大规模平行数据来训练父模型，然后将该父模型直接应用在零标注的翻译任务上。在两个公共数据集上的实验表明，我们的方法明显优于基于桥接的baseline系统和各种多语言NMT方法。

<br>

<h4 id='2'>Introduction</h4>
这篇文章主要是为了进行`zero-shot`的两种语言的翻译，作者认为零标注场景的翻译失败主要来自于语言空间的不匹配（`domain shift problem`）。而一般的迁移学习方法并不等保证source和pivot语言的特征分布式相似的，所以子模型在一些场景下可能从父模型身上学不到什么有用的东西。

这篇文章提出的方法建立在这样一个zero-shot的场景下：没有source→target的训练语料，但是有很多source→pivot 以及 pivot→target的语料（`所以这种方法和普通的pivot learning比有什么优势？ `），作者提出的方法是期望用一种`跨语言的预训练方法`来解决提到的domain shift问题，然后再进行tranfer learning的方法训练语料。重点是他们提出的这个预训练方法**BRLM**。

> 主要步骤

1. 用source或pivot的单语语料或者source->pivot的双语语料预训练encoder（~~`双语encoder？训练一整个翻译模型然后只取encoder部分吗？`~~→`后面有讲到用到单语的这个是MLM，用到双语的是BRLM，TLM`）
2. 在训练好的encoder基础上训练pivot->target的翻译模型作为parent model。
   trick：freeze一些encoder层防止退化。
3. 通过parent model训练source->target端的NMT。

<br>

<h4 id='3'>Method</h4>
训练流程分为预训练阶段(pretraining phase)和迁移阶段(transfer phase)。

##### 预训练阶段

作者通过之前的两种方法：MLM、TLM，提出了自己的方法BRLM，又根据对齐方式的不同分为了BRLM-HA（Hard Aligment）与BRLM-SA（Soft Alignment）。

> **MLM**

灵感来源于Bert，在单语语料中，训练模型，随机[MASK]语料中的单词让模型进行预测。将多种语言的语料作为输入时，这种方法能找到不同语言中共享的特征，通过这种方法，可以将不同语言的词语表示映射到一个公共空间中。(`为啥这么大本事`)

> **TLM**

TLM是MLM的一个扩展，TLM将双语句对连接成一整个句子进行Mask模式的模型训练。 

> **BRLM**(BRige Language Modeling)

它的思路是如果两端的语言空间足够相近的话，一句中被Mask的单词也可以通过另一端对齐的词语的上下文推断出。根据不同的对齐方法，将BRLM分为了两种。

- BRLM-HA

  1. 外部对齐工具进行source->pivot的对齐
  2. 利用Transformer训练source->pivot的encoder
     训练过程：随机mask source句中的单词，通过上下文以及pivot端对齐的单词推断这个词。（也做pivot端的mask，反过来同理）

- BRLM-SA

  将一个额外的attention层加入，用于获取对齐信息。这样做的好处就是可以避免外部对齐工具产生的错误，并且支持多对一的对齐方式。

作者指出，由于BRLM能更好地利用对齐信息，所以能够获得更准确地词语级别的对齐表示，因此能够更好的解决domain shift 问题。

(`所以预训练里说白了就是一个考虑对齐的Bert？`)

<br>

##### 迁移阶段

1. 先用MLM训练单语的source和pivot(`相当于用Bert做Embedding吧`)

2. 用所得接着训练TLM/BRLM(`这一步的训练是相当于得到了source→pivot的encoder作为parent model`)

3. 用得到的encoder参数初始化pivot→target model
   训练过程中freeze一些encoder的层防止退化.

   <br>

<h4 id='4'>实验</h4>
 <br>

> **性能**

![img]({{ site.baseurl | prepend:site.url}}/images/p3-table1.png){: .center-image }*Europarl 数据集*

字典是用60K BPE做的，中间语言选的都是英语，可以看出MLM+BRLM-SA最好。

![img]({{ site.baseurl | prepend:site.url}}/images/p3-table2.png){: .center-image }*MultiUN数据集*

字典是80K BPE ,所有句都通过moses的一个工具分了词。（`嗯？不是用bpe的分词结果吗`），害怕词典过大做了全文转小写。

MLM+BRLM-SA最好仍然是最好的（加了BT之后，普通的结果比不太上baseline）。

> **句子表示**

![img]({{ site.baseurl | prepend:site.url}}/images/p3-pic1.png){: .center-image }*Encoder 的余弦相似度*

MLM+BRLM-SA是最平稳的，而且相似度高，说明我们得到了非常高质量的跨语言句子表示。（`为什么平稳代表优秀？平稳代表训练的意义不大啊，说明一开始就很好了，之后也没怎么变过？`）

> **上下文词语表示**

![img]({{ site.baseurl | prepend:site.url}}/images/p3-pic2.png){: .center-image }*词语的余弦相似度*

越亮说明越相似，同样MLM+BRLM-SA的对角线是最亮的。

> **冻结参数**

实验证明，冻结transformer的前4层效果最好。

<br>

<h4 id='5'>结论与贡献</h4>
作者主要的贡献是为了解决语义空间差距的问题，设计了一个MLM式的跨语言预训练模型，包含对齐信息，最后用于pivot的transfer learning。

> **思考**

1. 最后一个表感觉有些不公平，没有将其他方法也加BT，单纯的方法好像是打不过第三个Baseline的。
2. 没什么实操价值，如果考虑中文，只要pivot的话应该都是考虑同字母集的吧，中文应该做不了吧。
3. 刷一刷transfer Learning的文章看看还能不能发现什么吧，从这篇文章看来应该是有一种基于pivot的transfer Learning的固有方法。