---
layout: post
title: "论文笔记(3)利用基于跨语言预训练的迁移学习方法进行零标注的神经机器翻译"
date: 2020-03-01 11:48:18
description: Cross-lingual Pre-training Based Transfer for Zero-shot Neural Machine Translation
tags: 
 - Machine Translation
 - NLP
 - Low Resource
 - Transfer learning
 - Zero-shot
 - 论文笔记
---



#### 论文信息

> 题名: Cross-lingual Pre-training Based Transfer for Zero-shot Neural Machine Translation
>
> 来源: AAAI
>
> 年份: 2020
>
> 学习原因: 组会备选

* ##### [1 Abstract](#1)

* ##### [2 Introduction](#2)

* ##### [3 Method](#3)

* ##### [4 Experiments](#4)

* ##### [5 Conclusions & Contibutions](#5)

---

<br>

<h4 id='1'>摘要</h4>

不同语言对之间的迁移学习在低资源场景下对神经机器翻译（NMT）具有一定效果。然而，由于源端的迁移对象（父模型）和被迁移对象（子模型）之间的**语义空间不匹配**问题，目前的迁移方法在零标注等极端场景下并不有效。为了解决该问题，作者提出一种全新的基于跨语言预训练的迁移方法。关键思想是让所有源语言`共享相同的特征空间`，从而实现零标注翻译的平稳过渡。作者引入了一种仅基于单语和两种基于双语的跨语言预训练方法，以获得适用于不同语言的通用encoder。进一步地，作者利用该通用编码器在大规模平行数据来训练父模型，然后将该父模型直接应用在零标注的翻译任务上。在两个公共数据集上的实验表明，我们的方法明显优于基于桥接的baseline系统和各种多语言NMT方法。

<br>

<h4 id='2'>Introduction</h4>

这篇文章 

<br>

<h4 id='3'>Method</h4>



<br>

<h4 id='4'>实验</h4>



 <br>

<h4 id='5'>结论与贡献</h4>



**贡献**

- 提升了NMT baseline的BLEU
- 通过迁移学习的模型做re-scoring提升了SBMT的BLEU