---
layout: post
title: "论文笔记(2)利用迁移学习进行低资源的神经机器翻译"
date: 2020-02-26 12:45:54
description: Transfer Learning for Low-Resource Neural Machine Translation
tags: 
 - Machine Translation
 - NLP
 - Low Resource
 - Transfer learning
 - 论文笔记
---



#### 论文信息

> 题名: Transfer Learning for Low-Resource Neural Machine Translation
>
> 来源: EMNLP
>
> 年份: 2016
>
> 学习原因: 学习AAAI新的论文中的迁移学习方法

* ##### [1 Abstract](#1)

* ##### [2 Introduction](#2)

* ##### [3 Method](#3)

* ##### [4 Experiments](#4)

* ##### [5 Conclusions & Contibutions](#5)

---

<br>

<h4 id='1'>摘要</h4>

在语料丰富的情况下，基于encoder-decoder结构的NMT非常强力有效，但是在低资源语言的翻译下就不再那么好用了。作者提出用资源丰富的语料对训练parent model，然后将所学到的一些参数迁移到低资源的模型(child model)中。用这种方法，他们将原有的NMT model baseline的BLEU值提升了5.6，通过未知词替换也提高了BLEU值2点，并且其性能可以SBMT相提并论，并且在其中一种低资源语言的语言对上的翻译超过了SBMT。他们同时发现通过NMT计算SBMT中的最终分数能够帮助SBMT的翻译得到提升，可以达到SOTA。

<br>

<h4 id='2'>Introduction</h4>

作者首次将Transfer Learning应用到了翻译当中。主要的思想是训练一个高资源平行语料对的翻译模型（parent model）去初始化低资源平行语料模型（child model）的参数。最后固定parent model的特定参数对剩余的参数进行微调得到最终模型。

作者发现这种方法不仅能提高原有NMT baseline的BLEU值，还能通过对SOTA的SBMT模型做re-score，来提升其翻译效果。

<br>

<h4 id='3'>Method</h4>

结构：两层RNN LSTM（`当时还没有Transformer哈哈`）

数据特征：训练集规模是真的很小（0.2M），以及来自众多不同领域。

![img]({{ site.baseurl | prepend:site.url}}/images/p2-table2.png){: .center-image }*Table1 低资源语料大小*

方法：

1. 用双向资源都丰富的平行语料训练NMT（法-英），得到parent model。
2. 用得到的parent model初始化一个新的NMT的参数，称为child model
3. 用低资源的平行语料训练child model（低-英）（`是否代表低资源的一定要当源语言这样训练？`）
   **fine-tune阶段**：对一些参数固定，另一些进行fine-tune，可以看作是对参数空间里的参数进行正则化的近似。

<br>

<h4 id='4'>实验</h4>

**Transfer Learning**

![img]({{ site.baseurl | prepend:site.url}}/images/p2-table1.png){: .center-image }*Table2 迁移学习实验*

通过作者提出的迁移学习方法训练出的模型`Xfer`对比baseline有了较大的提升（5.6 BLEU）。通过位知词替换方法，最终模型`Final`的平均BLEU值提升了7.5，并在一种语言（Hausa）翻译的表现上超过了SBMT。

<br>

**Re-scoring**

![img]({{ site.baseurl | prepend:site.url}}/images/p2-table3.png){: .center-image }*Tabel3 SBMT的Re-score实验*

当对SBMT系统的输出n个最佳列表（n = 1000）做Re-scoring时，作者使用这个带有转移学习功能的NMT模型。（`不太了解SBMT的这一步`）

其中的`LM`是作为没有经过transfer的NMT model作对照的语言模型。

<br>

**Fine-tune & Fronzen**

在进行child model的训练时，初始化参数时会将一些参数（可以在新模型中延续使用的）frozen，比如英文的Embedding，其它参数进行fine-tune，作者比较了几种fine-tune和frozen不同参数的组合策略，作为消融实验，提升效果如图。

![img]({{ site.baseurl | prepend:site.url}}/images/p2-table4.png){: .center-image }*Table4 Ablation*

<h4 id='5'>结论与贡献</h4>

通过作者继续做的一堆实验，分析出Transfer的一些规律：

- 当parent model 中的高资源语料和child model中的语料在语言学上更相似时，transfer的效果更好。
- parent model中的语言选择对child model的模型效果影响至关重要。
- 通过Transfer learning，child model学到的不仅仅是英语的语言模型，还能通过parent的大量双语文本学到其一些可以利用的翻译参数。（`玄学`）

**贡献**

- 提升了NMT baseline的BLEU
- 通过迁移学习的模型做re-scoring提升了SBMT的BLEU