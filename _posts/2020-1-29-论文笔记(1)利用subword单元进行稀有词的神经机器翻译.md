---
layout: post
title: "论文笔记(1)利用subword进行稀有词的神经机器翻译"
date: 2020-01-29 13:57:51
description: Neural Machine Translation of Rare Words with Subword Units
tags: 
 - Machine Translation
 - NLP
 - BPE
 - 论文笔记
---



#### 论文信息

> 题名: Neural Machine Translation of Rare Words with Subword Units
>
> 来源: 懒得查
>
> 年份: 2016
>
> 学习原因: 学习BPE

* ##### [1 Abstract](#1)

* ##### [2 Motivation](#2)

* ##### [3 Related Work](#3)

* ##### [4 Algorithm](#4)

* ##### [5 Experiments](#5)

* ##### [6 Analysis and Contribution](#6)

---

<br>

<h4 id='1'>摘要</h4>

现有的NMT通常都是通过一个定长词典进行翻译，而真实情形的翻译问题的词典是开放的。为了解决稀有词和未登录词的翻译问题，常见的做法是设置一个back-off词表，每当翻译到原有的词典无法翻译的词时，就去查这个词表。这篇论文提出了一种更简单的方法——在分词阶段将词分成更小的子词单元，通过这种方法增加词典对稀有词和未登录词的覆盖，从而提升翻译性能。作者的方法使WMT15上的英→德以及英→俄翻译的BLEU值分别增加了1.1及1。

<br>

<h4 id='2'>Motivation</h4>

解决翻译过程中的OOV问题。

**直觉**：很多类型的词拆成更小单元后也是可翻译的。许多经验丰富的译者即使是第一次见到某个词时，通过该词的结构组成，如词素(morpheme)或音素(phoneme)，也可以大概猜测出该词的意思。。

**可拆分的词列举**：

- 命名实体（共享字母表直接拷贝，不共享则通过音节进行音译）
- 同源外来词（基于char级别翻译即可）
- 形态复杂的词 （复合词可能由多个词组成，或有可翻译的前后缀）

另外，NMT模型通常都会用到**注意力机制**，在word-level的模型中，模型每次只能计算在word级别上的注意力，本文作者希望模型可以在每一步学习中将注意力放在不同的subword上，显然这样更有意义和效率。

<br>

<h4 id='3'>相关工作</h4>

对于OOV问题的解决方法，前人也有许多探讨。例如，将人名直接进行拷贝或进行音译，另外，在SMT中也有不少工作讨论关于分割单词的语素的研究工作，但是较为保守。
通过相关工作的介绍，作者对分割子词给出以下意见：

- 分割单词的语素的办法最好是基于任务的，并且为每个语素和字符学习固定长度的向量。
- NMT 模型的注意力机制能够作用在子词上。
- 为了能权衡词库大小和句子长度，往往只将句子中不常见单词进行分割。

<br>

<h4 id='4'>算法</h4>

```python
import re, collections
def get_stats(vocab):
    pairs = collections.defaultdict(int)
    for word, freq in vocab.items():
        symbols = word.split()
        for i in range(len(symbols)-1):
            pairs[symbols[i],symbols[i+1]] += freq
    return pairs

def merge_vocab(pair, v_in):
    v_out = {}
    bigram = re.escape(' '.join(pair))
    p = re.compile(r'(?<!\S)' + bigram + r'(?!\S)')
    for word in v_in:
        w_out = p.sub(''.join(pair), word)
        v_out[w_out] = v_in[word]
    return v_out

vocab = {'l o w </w>' : 5, 'l o w e r </w>' : 2, 'n e w e s t </w>':6,'w i d e s t </w>':3}
num_merges = 10
for i in range(num_merges):
    pairs = get_stats(vocab)
    best = max(pairs, key=pairs.get)
    vocab = merge_vocab(best, vocab)
    print(best)
```

算法很简单，思路是将词拆成字符，将共现频率高的pair连接起来。

作者同时提出了两种在翻译中应用BPE的方案：

1.原文和译文分别做BPE。

2.`Joint BPE`作者实验的方案是简单的将原文和译文的训练集拼在一起做BPE。

作者指出方案1会使词汇表的大小更小，并且在训练过程中，对应语言的subword能够轻易的被找出，而第2种方案会提升源语言和目标语言分词的一致性。

<br>

<h4 id='5'>实验</h4>

**实验目标**

- 使用BPE能否提高未登录词和训练语料中未出现单词的翻译质量？即希望用一个固定大小的子词字典来表示开放的词库，如此就能高效的训练和解码。
- 词库大小、句子长度和翻译质量，三者之间如何权衡？

**评估方法**

- 子词个数的相关统计 
  比较了不同方法对词库大小和未登录词个数的影响，character n-grams、compound splitting、morfessor、hyphenation和BPE等方法。实验发现BPE是有效的，并且达到了测试集中没有未登录词的需求。
- 翻译实验
  - BLEU 翻译准确率
  - CHRF3 更偏向召回率
  - Unigram F1 用来衡量稀有单词和未登录词的翻译质量

<br>

<h4 id='6'>分析与贡献</h4>

- 将稀有词和未登录词表示成子词序列
- BPE算法能处理具有多样形态的词以及复合词多的语言的翻译策略
  